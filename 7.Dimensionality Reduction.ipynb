{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9bc6dc6",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15dc23d1",
   "metadata": {},
   "source": [
    "### Q173. What is curse of dimensionality?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be05c0c8",
   "metadata": {},
   "source": [
    "The curse of dimensionality refers to the fact that many problems that do not exist in low-dimensional space arise in high-dimensional space. In Machine Learning, one common manifestation is the fact that randomly sampled high-dimensional vectors are generally far from one another, increasing the risk of overfitting and making it very difficult to identify patterns without having plenty of training data."
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "4c1a5c4f",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad2405a",
   "metadata": {},
   "source": [
    "### Q174. What is advantage of dimensionality reduction?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ab83ac",
   "metadata": {},
   "source": [
    "Dimensionality reduction has several advantages, including:\n",
    "\n",
    "     1.Data compression: Reducing the number of features in a dataset can help reduce storage and computational requirements.\n",
    "\n",
    "    2.Noise reduction: Removing redundant or irrelevant features can reduce the impact of noise in the data.\n",
    "\n",
    "    3.Visualization: High-dimensional data is difficult to visualize, but reducing the number of dimensions can make it easier to plot and understand the relationships between points.\n",
    "\n",
    "    4.Improved model performance: In some cases, reducing the number of features can improve the performance of machine learning models by removing redundant or noisy features that might confuse the model.\n",
    "\n",
    "    5.Feature selection: Dimensionality reduction can be used as a method of feature selection, where important features are retained while less important ones are discarded."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7679c47a",
   "metadata": {},
   "source": [
    "### Q175. Explain the projection technique."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3234dc96",
   "metadata": {},
   "source": [
    "Projection is a technique used in dimensionality reduction to map high-dimensional data onto a lower-dimensional space while preserving as much of the information as possible.\n",
    "\n",
    "The goal of projection is to find a new representation of the data that captures its most important features and relationships. This is achieved by projecting the data points onto a lower-dimensional subspace.\n",
    "\n",
    "There are two main types of projection techniques: linear and non-linear. Linear projection techniques include Principal Component Analysis (PCA) and Linear Discriminant Analysis (LDA), which project the data onto a line or a plane, respectively. Non-linear projection techniques, such as t-SNE, project the data onto a non-linear surface to capture non-linear relationships in the data.\n",
    "\n",
    "The choice of projection technique depends on the structure of the data and the problem being solved. For example, PCA is a popular choice for data compression and visualization, while LDA is used for classification problems.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0529b64",
   "metadata": {},
   "source": [
    "### Q176. For what kind of dataset is the projection technique unsuitable?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdfbf465",
   "metadata": {},
   "source": [
    "Projection techniques are not suitable for all types of datasets. Here are a few cases where projection techniques might not be appropriate:\n",
    "\n",
    "Non-linearly separable data: If the data points cannot be separated by a line or a plane, non-linear projection techniques like t-SNE may be more appropriate.\n",
    "\n",
    "Sparse data: Projection techniques work well with dense data, where most of the features are non-zero. However, if the data is sparse, the projected data may not retain enough information to be useful.\n",
    "\n",
    "High noise levels: If the data contains a high level of noise, projection techniques may not provide meaningful results as they will capture the noise along with the signal.\n",
    "\n",
    "Unstructured data: Projection techniques are designed for structured data, where the features have a clear meaning. For unstructured data, such as images or audio, other techniques like convolutional neural networks may be more appropriate.\n",
    "\n",
    "In summary, the suitability of projection techniques depends on the structure and quality of the data, and it's important to consider these factors when choosing a dimensionality reduction technique."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478cb43d",
   "metadata": {},
   "source": [
    "### Q177. What is mainfold learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eed04a0",
   "metadata": {},
   "source": [
    "Manifold learning is a technique used in machine learning to uncover the underlying structure of high-dimensional data. The idea behind manifold learning is that many high-dimensional datasets lie on or close to a low-dimensional manifold, a curved or curved surface embedded in a higher-dimensional space.\n",
    "\n",
    "The main goal of manifold learning is to map the data points from a high-dimensional space onto a lower-dimensional manifold that captures the most important relationships and patterns in the data. This can be useful for tasks such as visualization, dimensionality reduction, and clustering.\n",
    "\n",
    "There are several popular algorithms for manifold learning, including t-SNE, Isomap, Laplacian Eigenmaps, and Locally Linear Embedding (LLE). These algorithms work by finding a mapping from the high-dimensional data to a lower-dimensional representation that preserves the local relationships between the data points.\n",
    "\n",
    "Manifold learning is widely used in computer vision, natural language processing, and other areas of machine learning to extract useful information from high-dimensional data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae44ba0",
   "metadata": {},
   "source": [
    "### Q178. What is manifold assumption?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d602d0c",
   "metadata": {},
   "source": [
    "The manifold assumption is a fundamental idea in machine learning and computer vision that states that many high-dimensional datasets lie on or near a lower-dimensional manifold. The manifold assumption asserts that the underlying structure of high-dimensional data is often not random, but can be modeled as a smooth, low-dimensional surface.\n",
    "\n",
    "In other words, the manifold assumption assumes that the relationship between the features in a high-dimensional dataset can be captured by a much lower number of underlying dimensions. This assumption allows machine learning algorithms to extract meaningful patterns and relationships from high-dimensional data.\n",
    "\n",
    "For example, images of handwritten digits can be represented as a set of pixels in a high-dimensional space, but it is believed that the underlying structure of the images can be captured by a much lower number of dimensions, such as the curvature of the lines, the thickness of the strokes, and so on.\n",
    "\n",
    "The manifold assumption has been proven to be useful in many applications, including image classification, computer vision, and natural language processing, and has been the basis for many successful machine learning algorithms, such as t-SNE, Isomap, Laplacian Eigenmaps, and Locally Linear Embedding (LLE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431e2d0c",
   "metadata": {},
   "source": [
    "### Q179. What is PCA (Principal Component Analysis)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a7f3a1",
   "metadata": {},
   "source": [
    "Principal Component Analysis (PCA) is a linear dimensionality reduction technique used in machine learning to reduce the number of features in a dataset while retaining as much of the information as possible. PCA is a widely used method for data compression, visualization, and feature extraction.\n",
    "\n",
    "The idea behind PCA is to transform the high-dimensional data into a lower-dimensional space such that the first transformed feature, called the first principal component, captures as much of the variance in the data as possible, the second transformed feature, called the second principal component, captures as much of the remaining variance as possible, and so on. The transformed features are orthogonal to each other and represent a new coordinate system that captures the most important relationships and patterns in the data.\n",
    "\n",
    "PCA works by finding the eigenvectors of the covariance matrix of the data, and these eigenvectors are used to form the principal components. The principal components are then used to transform the data into a lower-dimensional space.\n",
    "\n",
    "In summary, PCA is a fast and efficient method for reducing the dimensionality of a dataset while retaining its important features. It is a useful technique for data compression, visualization, and feature extraction, and is widely used in many fields, including computer vision, natural language processing, and genomics.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e1781a",
   "metadata": {},
   "source": [
    "### Q180. Explain how PCA reduces the dimension of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba84a09",
   "metadata": {},
   "source": [
    "Principal Component Analysis (PCA) reduces the dimension of the data by transforming it from a high-dimensional space into a lower-dimensional space while preserving as much of the information as possible. Here's how PCA works:\n",
    "\n",
    "Centering the data: The first step in PCA is to center the data by subtracting the mean of each feature from each data point. This step ensures that the first principal component captures the variation in the data, rather than just the mean of the data.\n",
    "\n",
    "Covariance matrix: The next step is to compute the covariance matrix of the centered data. The covariance matrix is a square matrix that contains the pairwise covariance between all the features in the data.\n",
    "\n",
    "Eigenvectors and eigenvalues: The covariance matrix is then diagonalized by finding its eigenvectors and eigenvalues. The eigenvectors represent the directions in which the data varies the most, and the eigenvalues represent the magnitude of the variation along each direction.\n",
    "\n",
    "Principal components: The eigenvectors with the largest eigenvalues are chosen to form the principal components, which represent a new coordinate system for the data. The first principal component captures the maximum amount of variation in the data, the second principal component captures the maximum amount of remaining variation, and so on.\n",
    "\n",
    "Dimensionality reduction: The final step is to transform the data into the lower-dimensional space defined by the principal components. This is done by projecting the data onto the principal components, resulting in a lower-dimensional representation of the data. The number of dimensions in the lower-dimensional representation can be chosen based on the desired level of information retention.\n",
    "\n",
    "In summary, PCA reduces the dimension of the data by transforming it into a lower-dimensional space defined by the principal components, which capture the most important relationships and patterns in the data. This allows PCA to reduce the dimensionality of the data while preserving as much of the information as possible.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6f2865",
   "metadata": {},
   "source": [
    "### Q181. What is the use of Principal Components in PCA?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e72d1b8",
   "metadata": {},
   "source": [
    "Principal Components (PCs) are the primary building blocks of Principal Component Analysis (PCA), a dimensionality reduction technique used in machine learning. The PCs are linear combinations of the original features in the data, and they represent a new coordinate system that captures the most important relationships and patterns in the data.\n",
    "\n",
    "**The use of PCs in PCA is two-fold:**\n",
    "\n",
    "    Data compression: The PCs can be used to reduce the dimension of the data by projecting it onto a lower-dimensional space defined by the PCs. The number of PCs used for the projection can be chosen based on the desired level of information retention, allowing PCA to perform data compression while preserving as much of the information as possible.\n",
    "\n",
    "    Feature extraction: The PCs can also be used to extract meaningful features from the data that can be used as input to machine learning algorithms. The PCs capture the most important patterns and relationships in the data, and they can be used to identify the underlying structure of the data, even if it is complex and high-dimensional.\n",
    "\n",
    "    In summary, the PCs in PCA are used to reduce the dimension of the data and to extract meaningful features from it. These features can be used as input to machine learning algorithms, allowing PCA to perform data compression, visualization, and feature extraction in a fast and efficient manner."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a29b1c",
   "metadata": {},
   "source": [
    "### Q182. How can you find the principal components of a training set?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf996f4",
   "metadata": {},
   "source": [
    "The principal components (PCs) of a training set can be found using the following steps:\n",
    "\n",
    "1. Centering the data: The first step is to center the data by subtracting the mean of each feature from each data point. This step ensures that the first principal component captures the variation in the data, rather than just the mean of the data.\n",
    "\n",
    "2. Covariance matrix: The next step is to compute the covariance matrix of the centered data. The covariance matrix is a square matrix that contains the pairwise covariance between all the features in the data.\n",
    "\n",
    "3. Eigenvectors and eigenvalues: The covariance matrix is then diagonalized by finding its eigenvectors and eigenvalues. The eigenvectors represent the directions in which the data varies the most, and the eigenvalues represent the magnitude of the variation along each direction.\n",
    "\n",
    "4. Principal components: The eigenvectors with the largest eigenvalues are chosen to form the principal components, which represent a new coordinate system for the data. The first principal component captures the maximum amount of variation in the data, the second principal component captures the maximum amount of remaining variation, and so on.\n",
    "\n",
    "In summary, the principal components of a training set can be found by computing the covariance matrix of the centered data, diagonalizing the covariance matrix to find its eigenvectors and eigenvalues, and selecting the eigenvectors with the largest eigenvalues to form the principal components. These principal components represent a new coordinate system that captures the most important relationships and patterns in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c92ff0",
   "metadata": {},
   "source": [
    "### Q183. What is explained variance ratio?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c5d47b",
   "metadata": {},
   "source": [
    "The explained variance ratio is a measure used in Principal Component Analysis (PCA) to quantify the amount of variation in the data that is explained by each principal component. It is defined as the ratio of the explained variance to the total variance of the data.\n",
    "\n",
    "In PCA, each principal component represents a new coordinate system that captures the most important relationships and patterns in the data. The explained variance ratio of a principal component is a measure of how much of the total variance in the data is captured by that component. The explained variance ratio is calculated as the ratio of the eigenvalue associated with a principal component to the sum of all the eigenvalues, which represents the total variance in the data.\n",
    "\n",
    "The explained variance ratio can be used to choose the number of principal components to use for dimensionality reduction. For example, if the goal is to preserve 95% of the total variance in the data, the first few principal components with the highest explained variance ratios can be selected until the sum of the ratios reaches 95%. This allows PCA to perform data compression while preserving as much of the information as possible.\n",
    "\n",
    "In summary, the explained variance ratio is a measure of the amount of variation in the data that is explained by each principal component in PCA, and it can be used to choose the number of components to use for dimensionality reduction.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b0a120",
   "metadata": {},
   "source": [
    "### Q184. How can you choose the right number of dimension of reduction?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d69fe89",
   "metadata": {},
   "source": [
    "Choosing the right number of dimensions to reduce a dataset to is an important step in Principal Component Analysis (PCA). There are several methods that can be used to determine the optimal number of dimensions, including:\n",
    "\n",
    "Scree Plot: A scree plot is a graphical representation of the explained variance ratio for each principal component. The plot shows the cumulative explained variance as a function of the number of components used. The optimal number of dimensions is typically chosen at the \"elbow\" of the plot, which is the point where the explained variance plateaus.\n",
    "\n",
    "Explained Variance Ratio: The explained variance ratio is a measure of the amount of variation in the data that is explained by each principal component. The optimal number of dimensions can be chosen as the minimum number of components that explains a desired amount of variance, for example 95% of the total variance.\n",
    "\n",
    "Cross-Validation: Cross-validation is a technique that can be used to estimate the generalization performance of a machine learning model. The optimal number of dimensions can be chosen as the number that gives the best generalization performance as determined by cross-validation.\n",
    "\n",
    "Reconstruction Error: The reconstruction error is a measure of the difference between the original data and the reconstructed data after dimensionality reduction. The optimal number of dimensions can be chosen as the minimum number of dimensions that results in an acceptable reconstruction error.\n",
    "\n",
    "In summary, there are several methods that can be used to determine the optimal number of dimensions to reduce a dataset to, including the scree plot, explained variance ratio, cross-validation, and reconstruction error. The method used will depend on the specific requirements of the problem and the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba7802b",
   "metadata": {},
   "source": [
    "### Q185. How can you use PCA of data compression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edc5cd6",
   "metadata": {},
   "source": [
    "Principal Component Analysis (PCA) can be used for data compression by reducing the number of dimensions in a dataset while preserving as much of the information as possible. The steps for using PCA for data compression are as follows:\n",
    "\n",
    "Center the data: The first step is to center the data by subtracting the mean of each feature from each data point. This step ensures that the first principal component captures the variation in the data, rather than just the mean of the data.\n",
    "\n",
    "Compute the covariance matrix: The next step is to compute the covariance matrix of the centered data. The covariance matrix is a square matrix that contains the pairwise covariance between all the features in the data.\n",
    "\n",
    "Diagonalize the covariance matrix: The covariance matrix is then diagonalized by finding its eigenvectors and eigenvalues. The eigenvectors represent the directions in which the data varies the most, and the eigenvalues represent the magnitude of the variation along each direction.\n",
    "\n",
    "Select the principal components: The eigenvectors with the largest eigenvalues are chosen to form the principal components, which represent a new coordinate system for the data. The first principal component captures the maximum amount of variation in the data, the second principal component captures the maximum amount of remaining variation, and so on.\n",
    "\n",
    "Choose the number of dimensions: The optimal number of dimensions to reduce the data to can be determined using a method such as the scree plot, explained variance ratio, cross-validation, or reconstruction error.\n",
    "\n",
    "Transform the data: Finally, the data can be transformed into the new coordinate system defined by the principal components, effectively reducing the number of dimensions. The transformed data can be used for further analysis or as a more compact representation of the original data.\n",
    "\n",
    "In summary, PCA can be used for data compression by transforming the data into a new coordinate system defined by a smaller number of principal components, which capture the most important relationships and patterns in the data. This allows the data to be reduced to a smaller number of dimensions while preserving as much of the information as possible.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380a3e0a",
   "metadata": {},
   "source": [
    "### Q186. Compress the (MNIST data)[https://www.kaggle.com/datasets/oddrationale/mnist-in-csv] using PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4012329e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "# Load MNIST data\n",
    "mnist = fetch_openml('mnist_784', version=1)\n",
    "\n",
    "# Perform PCA with n_components=0.95\n",
    "pca = PCA(n_components=0.95)\n",
    "pca.fit(mnist.data)\n",
    "\n",
    "# Transform the data using PCA\n",
    "mnist_compressed = pca.transform(mnist.data)\n",
    "\n",
    "# Compressed data has lower dimensionality\n",
    "print(\"Compressed data shape:\", mnist_compressed.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711edeeb",
   "metadata": {},
   "source": [
    "### Q187. What is Randomized PCA?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810f145b",
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "source": [
    "Randomized PCA is a fast and efficient variant of Principal Component Analysis (PCA) that approximates the principal components of a large dataset. Instead of computing the full eigenvalue decomposition of the covariance matrix, which can be computationally intensive for large datasets, randomized PCA selects a smaller set of basis vectors and projects the data onto this reduced basis. The new basis vectors are then used to approximate the principal components of the original data.\n",
    "\n",
    "Randomized PCA relies on random matrix theory to provide a probabilistic guarantee of finding the principal components with high accuracy, while avoiding the computational burden of computing the full eigenvalue decomposition. The procedure can be iterated to improve the approximation and increase accuracy.\n",
    "\n",
    "The main advantage of randomized PCA over traditional PCA is its scalability. It allows PCA to be applied to datasets with millions of data points and hundreds of features, which would otherwise be computationally intractable using traditional PCA. Additionally, randomized PCA can be parallelized, further reducing the computational time and making it more efficient for large datasets.\n",
    "\n",
    "In summary, randomized PCA is a fast and efficient variant of PCA that approximates the principal components of a large dataset by projecting the data onto a smaller set of basis vectors, reducing the computational complexity of traditional PCA and making it more suitable for large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e70b06",
   "metadata": {},
   "source": [
    "### Q188. What is Incremental PCA?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64c9c67",
   "metadata": {},
   "source": [
    "ncremental Principal Component Analysis (PCA) is a variation of PCA that allows for the computation of the principal components of a dataset incrementally, as new data points are added. This is useful in scenarios where the entire dataset cannot be loaded into memory, or where the data is constantly changing and needs to be updated.\n",
    "\n",
    "In incremental PCA, the principal components are computed in batches, rather than on the entire dataset at once. The batch size can be adjusted to control the trade-off between accuracy and computational efficiency. The principal components from each batch are combined to form a complete approximation of the principal components of the entire dataset.\n",
    "\n",
    "Incremental PCA is also useful for reducing the dimensionality of streaming data, as it allows for real-time analysis without the need to store the entire dataset. Additionally, incremental PCA can be used to update the principal components as the data changes, making it a useful tool for online learning applications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27409d2",
   "metadata": {},
   "source": [
    "In summary, Incremental PCA is a method for computing the principal components of a dataset in small, manageable chunks, rather than computing them all at once. It allows for real-time analysis of streaming data and is useful in scenarios where the entire dataset cannot be loaded into memory or the data is constantly changing and needs to be updated.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbb19f3",
   "metadata": {},
   "source": [
    "### Q189. Explain Kernel PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059d42f1",
   "metadata": {},
   "source": [
    "Kernel Principal Component Analysis (Kernel PCA) is a non-linear extension of PCA that is used for dimensionality reduction and feature extraction in complex, non-linearly separable datasets. Unlike traditional PCA, which operates in the original feature space, Kernel PCA operates in a transformed, higher-dimensional feature space known as the \"kernel trick\".\n",
    "\n",
    "The main idea behind Kernel PCA is to map the original data points into a higher-dimensional feature space using a non-linear transformation known as a kernel function. The mapping is performed in such a way that the new feature space has increased separability and allows for linear discrimination of the data points.\n",
    "\n",
    "Once the data has been mapped into the higher-dimensional feature space, traditional PCA can be applied to compute the principal components. These components represent a new set of features that capture the underlying structure of the data. The dimensionality reduction is performed by projecting the data points onto a lower-dimensional subspace defined by a subset of the principal components.\n",
    "\n",
    "Kernel PCA is commonly used in a variety of applications, including image processing, data visualization, and machine learning. The choice of kernel function is important and depends on the specific problem and dataset being analyzed. Some popular kernel functions include the radial basis function (RBF) and polynomial kernel functions.\n",
    "\n",
    "In summary, Kernel PCA is a non-linear extension of PCA that maps the original data into a higher-dimensional feature space using a kernel function, allowing for linear discrimination of the data points in complex, non-linearly separable datasets. The method can be used for dimensionality reduction and feature extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f419533",
   "metadata": {},
   "source": [
    "### Q190. How can you select the best kernel and hyperparamter for dimensionality reduction?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907313ee",
   "metadata": {},
   "source": [
    "The best kernel and hyperparameters for dimensionality reduction using Kernel PCA can be selected through various techniques, such as cross-validation, grid search, and random search.\n",
    "\n",
    "In cross-validation, the data is divided into training and validation sets, and the performance of different kernels and hyperparameters is evaluated on the validation set. The kernel and hyperparameters that produce the best results are then selected.\n",
    "\n",
    "In grid search, a set of possible kernels and hyperparameters are pre-defined and the performance of each combination is evaluated. The combination that produces the best results is then selected.\n",
    "\n",
    "In random search, a set of random combinations of kernels and hyperparameters are generated and evaluated, and the best combination is selected.\n",
    "\n",
    "It is important to note that the selection of the best kernel and hyperparameters depends on the specific data and problem at hand, so the techniques mentioned above may need to be adjusted or combined to obtain the best results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b865c7",
   "metadata": {},
   "source": [
    "### Q191. What is LLE (Locally Linear Embedding)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3047eff",
   "metadata": {},
   "source": [
    "Locally Linear Embedding (LLE) is a non-linear dimensionality reduction technique that preserves the local structure of the data. Unlike linear techniques like PCA, LLE finds a low-dimensional representation of the data by preserving the relationships between the nearest neighbors of each data point.\n",
    "\n",
    "LLE works by reconstructing each data point as a weighted sum of its nearest neighbors. The weights are optimized such that the reconstructed points are as close as possible to the original points, while still preserving the local relationships between them. The low-dimensional representation of the data is then obtained by projecting the optimized weights onto a lower-dimensional space.\n",
    "\n",
    "LLE is well-suited for preserving complex non-linear structures in the data, and is often used for visualizing and analyzing high-dimensional datasets in fields such as computer vision and pattern recognition.\n",
    "\n",
    "In summary, Locally Linear Embedding (LLE) is a non-linear dimensionality reduction technique that preserves the local structure of the data by finding a low-dimensional representation of the data that preserves the relationships between the nearest neighbors of each data point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db17fee6",
   "metadata": {},
   "source": [
    "### Q192. Expalin the working of LLE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c70cb62",
   "metadata": {},
   "source": [
    "Locally Linear Embedding (LLE) is a non-linear dimensionality reduction technique that works by preserving the local geometry of the data. The basic idea behind LLE is to identify the k nearest neighbors of each data point, and then to find a low-dimensional representation of these neighbors that is locally linear.\n",
    "\n",
    "The LLE algorithm starts by computing the weights that describe the linear relationship between each data point and its k nearest neighbors. These weights are used to compute a set of \"reconstructed\" data points that preserve the local geometry of the original data. The reconstructed data points are then used as the input to a standard linear dimensionality reduction technique, such as PCA.\n",
    "\n",
    "The key advantage of LLE is that it is capable of capturing non-linear structure in the data, unlike linear techniques such as PCA. By preserving the local geometry of the data, LLE can identify complex patterns and structures that may not be apparent in a linear representation.\n",
    "\n",
    "In summary, LLE works by finding a low-dimensional representation of the k nearest neighbors of each data point that is locally linear, and then using this representation as the input to a standard linear dimensionality reduction technique.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9baf44",
   "metadata": {},
   "source": [
    "### Q193. What is Random Projections?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4450f78",
   "metadata": {},
   "source": [
    "Random Projections is a dimensionality reduction technique that maps high-dimensional data onto a lower-dimensional subspace by using random projections. The idea behind random projections is to preserve the structure of the original data in the lower-dimensional representation.\n",
    "\n",
    "Random projections are created by multiplying the original data by a randomly generated matrix, where each entry of the matrix is chosen from a random distribution. The size of the randomly generated matrix is determined by the desired reduction in dimensionality.\n",
    "\n",
    "Random Projections is particularly useful when dealing with high-dimensional data, as it can significantly reduce the computational complexity of processing the data while preserving the essential structure of the data. Additionally, random projections can be used to reduce the storage requirements of high-dimensional data.\n",
    "\n",
    "In summary, Random Projections is a dimensionality reduction technique that uses random projections to map high-dimensional data onto a lower-dimensional subspace while preserving the structure of the original data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43aca4d",
   "metadata": {},
   "source": [
    "### Q194. What is MDS (Multi Dimensional Scaling)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e55a5e7",
   "metadata": {},
   "source": [
    "Multi-Dimensional Scaling (MDS) is a statistical technique used for dimensionality reduction and visualization of high-dimensional data. It seeks to preserve the pairwise distances between data points in a lower-dimensional representation, while minimizing the distortion from the original distances.\n",
    "\n",
    "MDS can be thought of as an inverse problem to computing the pairwise distances between data points. Given a set of pairwise distances between data points, MDS finds a low-dimensional representation that approximates these distances as closely as possible. The new representation can be used for visualization and analysis of the data.\n",
    "\n",
    "There are two main types of MDS: metric MDS and non-metric MDS. In metric MDS, the goal is to preserve the exact pairwise distances, while in non-metric MDS, the goal is to preserve only the relative ordering of the distances.\n",
    "\n",
    "MDS is often used in fields such as psychology, marketing, and social sciences to visualize and analyze complex relationships between data points. It is also commonly used in geography to produce maps that preserve the relative distances between locations.\n",
    "\n",
    "In summary, Multi-Dimensional Scaling (MDS) is a statistical technique for dimensionality reduction and visualization of high-dimensional data that seeks to preserve the pairwise distances between data points in a lower-dimensional representation.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973c7cf3",
   "metadata": {},
   "source": [
    "### Q195. What is Isomap?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1c06fb",
   "metadata": {},
   "source": [
    "Isomap is a non-linear dimensionality reduction technique that is used to embed high-dimensional data into a low-dimensional space while preserving the intrinsic geometry of the data. Isomap is based on the concept of \"geodesic distance,\" which is a measure of the shortest path between two points on a curved surface.\n",
    "\n",
    "Isomap works by constructing a neighborhood graph of the data points and computing the shortest path distances between all pairs of data points. These distances are then used to construct a low-dimensional representation of the data that preserves the geodesic distances between the data points.\n",
    "\n",
    "The main advantage of Isomap is its ability to preserve the global structure of the data in the low-dimensional representation, which is not possible with linear techniques like PCA. Isomap has been applied to a wide range of applications, including image and speech recognition, face recognition, and text classification.\n",
    "\n",
    "In summary, Isomap is a non-linear dimensionality reduction technique that maps high-dimensional data into a low-dimensional space while preserving the intrinsic geometry of the data.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd05c29e",
   "metadata": {},
   "source": [
    "### Q196. What is t-SNE (t-Distributed Stochastic Neighbor Embedding)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee126a5",
   "metadata": {},
   "source": [
    "t-SNE (t-Distributed Stochastic Neighbor Embedding) is a dimensionality reduction technique that is used for exploring and visualizing high-dimensional data. It maps high-dimensional data points into a low-dimensional space while preserving the relative distances between the data points as much as possible.\n",
    "\n",
    "t-SNE works by constructing a probability distribution over pairs of data points, and minimizing the difference between the pairwise probabilities in the high-dimensional space and the low-dimensional space. This is achieved by using the t-distribution to measure the similarities between data points.\n",
    "\n",
    "t-SNE is well-suited for exploring and visualizing complex, non-linear data structures and can be applied to a wide range of problems in areas such as image recognition, natural language processing, and genomics.\n",
    "\n",
    "One of the main benefits of t-SNE is its ability to reveal patterns and structures in high-dimensional data that are not easily visible with other dimensionality reduction techniques. This makes t-SNE a popular tool for exploratory data analysis, as well as for visualizing and interpreting the results of complex machine learning algorithms.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d43d767",
   "metadata": {},
   "source": [
    "### Q197. What is LDA (Linear Discriminant Analysis)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4537733",
   "metadata": {},
   "source": [
    "Linear Discriminant Analysis (LDA) is a dimensionality reduction technique used for supervised learning. It is a linear transformation method that aims to reduce the dimensionality of the feature space to improve the discriminative power of the classifier. The main goal of LDA is to maximize the separation between different classes while minimizing the variance within each class.\n",
    "\n",
    "LDA works by computing the mean vectors of each class and then computing the between-class covariance matrix and the within-class covariance matrix. The eigenvectors of the covariance matrix are then used as the new basis for the transformed feature space. The new basis is selected such that the first few eigenvectors correspond to the largest eigenvalues, and therefore capture the most important structure in the data. The transformed feature space is then used to train a classifier, such as a linear discriminant classifier or a decision tree.\n",
    "\n",
    "LDA is particularly useful when the classes are well separated in the feature space and when the number of features is much greater than the number of samples. In these scenarios, LDA can help to reduce overfitting and improve the performance of the classifier.\n",
    "\n",
    "In summary, LDA is a supervised dimensionality reduction technique used to improve the discriminative power of the classifier. It works by transforming the feature space using the eigenvectors of the covariance matrix and is particularly useful for well-separated classes and high-dimensional data.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0819b10",
   "metadata": {},
   "source": [
    "### Q198. Once a datasets dimensionality has been reduced, is it possible to reverse the operation? If so, how? If not, why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f225e279",
   "metadata": {},
   "source": [
    "Once a dataset's dimensionality has been reduced using one of the algorithms we discussed, it is almost always impossible to perfectly reverse the operation, because some information gets lost during dimensionality reduction. Moreover, while some algorithms (such as PCA) have a simple reverse transformation procedure that can reconstruct a dataset relatively similar to the original, other algorithms (such as t-SNE) do not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935a3dc8",
   "metadata": {},
   "source": [
    "### Q199. Can PCA be used to reduce the dimensionality of a highly nonlinear dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf3f2b8",
   "metadata": {},
   "source": [
    "PCA can be used to significantly reduce the dimensionality of most datasets, even if they are highly nonlinear, because it can at least get rid of useless dimensions. However, if there are no useless dimensionsas in the Swiss roll datasetthen reducing dimensionality with PCA will lose too much information. You want to unroll the Swiss roll, not squash it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f549c38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9326972c",
   "metadata": {},
   "source": [
    "### Q200. Suppose you perform PCA on a 1,000-dimensional dataset, setting the explained variance ratio to 95%. How many dimensions will the resulting dataset have?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4a6196",
   "metadata": {},
   "source": [
    "That's a trick question: it depends on the dataset. Let's look at two extreme examples. First, suppose the dataset is composed of points that are almost perfectly aligned. In this case, PCA can reduce the dataset down to just one dimension while still preserving 95% of the variance. Now imagine that the dataset is composed of perfectly random points, scattered all around the 1,000 dimensions. In this case roughly 950 dimensions are required to preserve 95% of the variance. So the answer is, it depends on the dataset, and it could be any number between 1 and 950. Plotting the explained variance as a function of the number of dimensions is one way to get a rough idea of the dataset's intrinsic dimensionality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989b8282",
   "metadata": {},
   "source": [
    "### Q201. In what cases would you use vanilla PCA, Incremental PCA, Randomized PCA, or Kernel PCA?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25aedca8",
   "metadata": {},
   "source": [
    "Regular PCA is the default, but it works only if the dataset fits in memory. Incremental PCA is useful for large datasets that don't fit in memory, but it is slower than regular PCA, so if the dataset fits in memory you should prefer regular PCA. Incremental PCA is also useful for online tasks, when you need to apply PCA on the fly, every time a new instance arrives. Randomized PCA is useful when you want to considerably reduce dimensionality and the dataset fits in memory; in this case, it is much faster than regular PCA. Finally, Random Projection is great for very high-dimensional datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5c8622",
   "metadata": {},
   "source": [
    "### Q202. How can you evaluate the performance of a dimensionality reduction algorithm on your dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8943bc20",
   "metadata": {},
   "source": [
    "Intuitively, a dimensionality reduction algorithm performs well if it eliminates a lot of dimensions from the dataset without losing too much information. One way to measure this is to apply the reverse transformation and measure the reconstruction error. However, not all dimensionality reduction algorithms provide a reverse transformation. Alternatively, if you are using dimensionality reduction as a preprocessing step before another Machine Learning algorithm (e.g., a Random Forest classifier), then you can simply measure the performance of that second algorithm; if dimensionality reduction did not lose too much information, then the algorithm should perform just as well as when using the original dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134629ab",
   "metadata": {},
   "source": [
    "### Q203. Does it make any sense to chain two different dimensionality reduction algorithms?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49251ae",
   "metadata": {},
   "source": [
    "It can absolutely make sense to chain two different dimensionality reduction algorithms. A common example is using PCA or Random Projection to quickly get rid of a large number of useless dimensions, then applying another much slower dimensionality reduction algorithm, such as LLE. This two-step approach will likely yield roughly the same performance as using LLE only, but in a fraction of the time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d214fa0a",
   "metadata": {},
   "source": [
    "### Q204. Load the (MNIST data)[https://www.kaggle.com/datasets/oddrationale/mnist-in-csv] and split it into a training set and a test set (take the first 60,000 instances for training, and the remaining 10,000 for testing). Train a Random Forest classifier on the dataset and time how long it takes, then evaluate the resulting model on the test set. Next, use PCA to reduce the datasets dimensionality, with an explained variance ratio of 95%. Train a new Random Forest classifier on the reduced dataset and see how long it takes. Was training much faster? Next, evaluate the classifier on the test set. How does it compare to the previous classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d1cff85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "mnist = fetch_openml('mnist_784', as_frame=False)\n",
    "X_train, y_train = mnist.data[:60_000], mnist.target[:60_000]\n",
    "X_test, y_test = mnist.data[60_000:], mnist.target[60_000:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec61291",
   "metadata": {},
   "source": [
    "Exercise: _Train a Random Forest classifier on the dataset and time how long it takes, then evaluate the resulting model on the test set._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "215ae5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rnd_clf = RandomForestClassifier(n_estimators=100, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e092f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1min 8s\n",
      "Wall time: 1min 25s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(random_state=42)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time rnd_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8f69d30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9705"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = rnd_clf.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6333f7c",
   "metadata": {},
   "source": [
    "Exercise: _Next, use PCA to reduce the dataset's dimensionality, with an explained variance ratio of 95%._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ae38a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=0.95)\n",
    "X_train_reduced = pca.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4e3fac",
   "metadata": {},
   "source": [
    "Exercise: _Train a new Random Forest classifier on the reduced dataset and see how long it takes. Was training much faster?_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9eb645e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 3min 7s\n",
      "Wall time: 3min 43s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(random_state=42)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd_clf_with_pca = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "%time rnd_clf_with_pca.fit(X_train_reduced, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f559af",
   "metadata": {},
   "source": [
    "Oh no! Training is actually about twice slower now! How can that be? Well, as we saw in this chapter, dimensionality reduction does not always lead to faster training time: it depends on the dataset, the model and the training algorithm. See figure 8-6 (the `manifold_decision_boundary_plot*` plots above). If you try `SGDClassifier` instead of `RandomForestClassifier`, you will find that training time is reduced by a factor of 3 when using PCA. Actually, we will do this in a second, but first let's check the precision of the new random forest classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5b8506",
   "metadata": {},
   "source": [
    "Exercise: _Next evaluate the classifier on the test set: how does it compare to the previous classifier?_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6001130",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9481"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_reduced = pca.transform(X_test)\n",
    "\n",
    "y_pred = rnd_clf_with_pca.predict(X_test_reduced)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1f73de",
   "metadata": {},
   "source": [
    "It is common for performance to drop slightly when reducing dimensionality, because we do lose some potentially useful signal in the process. However, the performance drop is rather severe in this case. So PCA really did not help: it slowed down training *and* reduced performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00251d78",
   "metadata": {},
   "source": [
    "Exercise: _Try again with an `SGDClassifier`. How much does PCA help now?_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2fd3385a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 4min 44s\n",
      "Wall time: 5min 31s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SGDClassifier(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SGDClassifier</label><div class=\"sk-toggleable__content\"><pre>SGDClassifier(random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SGDClassifier(random_state=42)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "sgd_clf = SGDClassifier(random_state=42)\n",
    "%time sgd_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10a69e72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.874"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = sgd_clf.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8962a01",
   "metadata": {},
   "source": [
    "Okay, so the `SGDClassifier` takes much longer to train on this dataset than the `RandomForestClassifier`, plus it performs worse on the test set. But that's not what we are interested in right now, we want to see how much PCA can help `SGDClassifier`. Let's train it using the reduced dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57cc7940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 53.2 s\n",
      "Wall time: 1min\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SGDClassifier(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SGDClassifier</label><div class=\"sk-toggleable__content\"><pre>SGDClassifier(random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SGDClassifier(random_state=42)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd_clf_with_pca = SGDClassifier(random_state=42)\n",
    "%time sgd_clf_with_pca.fit(X_train_reduced, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ef2068",
   "metadata": {},
   "source": [
    "Nice! Reducing dimensionality led to roughly 5 speedup. :)  Let's check the model's accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a03f389",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8959"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = sgd_clf_with_pca.predict(X_test_reduced)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078830a7",
   "metadata": {},
   "source": [
    "Great! PCA not only gave us a 5 speed boost, it also improved performance slightly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8163c6",
   "metadata": {},
   "source": [
    "So there you have it: PCA can give you a formidable speedup, and if you're lucky a performance boost... but it's really not guaranteed: it depends on the model and the dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da68afa1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "81d43538",
   "metadata": {},
   "source": [
    "### Q205. Use t-SNE to reduce the MNIST dataset down to two dimensions and plot the                                        result using Matplotlib. You can use a scatterplot using 10 different colors to rep resent each images target class. Alternatively, you can replace each dot in the scatterplot with the corresponding instances class (a digit from 0 to 9), or even plot scaled-down versions of the digitimages themselves (if you plot all digits,the visualization will be too cluttered, so you should either draw a random sample or plot an instance only if no other instance has already been plotted at a close distance). You should get a nice visualization with well-separated clusters of digits. Try using other dimensionality reduction algorithms such as PCA, LLE, orMDS and compare the resulting visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "27b91e5d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_sample' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32m<timed exec>:1\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_sample' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(n_components=2, init=\"random\", learning_rate=\"auto\",\n",
    "            random_state=42)\n",
    "%time X_reduced = tsne.fit_transform(X_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cfd34a9",
   "metadata": {},
   "source": [
    "Now let's use Matplotlib's `scatter()` function to plot a scatterplot, using a different color for each digit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "54c95841",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m13\u001b[39m, \u001b[38;5;241m10\u001b[39m))\n\u001b[0;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mscatter(X_reduced[:, \u001b[38;5;241m0\u001b[39m], X_reduced[:, \u001b[38;5;241m1\u001b[39m],\n\u001b[0;32m      3\u001b[0m             c\u001b[38;5;241m=\u001b[39my_sample\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mint8), cmap\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjet\u001b[39m\u001b[38;5;124m\"\u001b[39m, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\n\u001b[0;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39maxis(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moff\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "plt.figure(figsize=(13, 10))\n",
    "plt.scatter(X_reduced[:, 0], X_reduced[:, 1],\n",
    "            c=y_sample.astype(np.int8), cmap=\"jet\", alpha=0.5)\n",
    "plt.axis('off')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d71f49",
   "metadata": {},
   "source": [
    "Isn't this just beautiful? :) Most digits are nicely separated from the others, even though t-SNE wasn't given the targets: it just identified clusters of similar images. But there is still a bit of overlap. For example, the 3s and the 5s overlap a lot (on the right side of the plot), and so do the 4s and the 9s (in the top-right corner)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0239c5b8",
   "metadata": {},
   "source": [
    "Let's focus on just the digits 4 and 9:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4df9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9, 9))\n",
    "cmap = plt.cm.jet\n",
    "for digit in ('4', '9'):\n",
    "    plt.scatter(X_reduced[y_sample == digit, 0], X_reduced[y_sample == digit, 1],\n",
    "                c=[cmap(float(digit) / 9)], alpha=0.5)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded0b961",
   "metadata": {},
   "source": [
    "Let's see if we can produce a nicer image by running t-SNE on just these 2 digits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccb4860",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = (y_sample == '4') | (y_sample == '9')\n",
    "X_subset = X_sample[idx]\n",
    "y_subset = y_sample[idx]\n",
    "\n",
    "tsne_subset = TSNE(n_components=2, init=\"random\", learning_rate=\"auto\",\n",
    "                   random_state=42)\n",
    "X_subset_reduced = tsne_subset.fit_transform(X_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f405297",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9, 9))\n",
    "for digit in ('4', '9'):\n",
    "    plt.scatter(X_subset_reduced[y_subset == digit, 0],\n",
    "                X_subset_reduced[y_subset == digit, 1],\n",
    "                c=[cmap(float(digit) / 9)], alpha=0.5)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c49d232",
   "metadata": {},
   "source": [
    "That's much better, although there's still a bit of overlap. Perhaps some 4s really do look like 9s, and vice versa. It would be nice if we could visualize a few digits from each region of this plot, to understand what's going on. In fact, let's do that now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9807538a",
   "metadata": {},
   "source": [
    "Exercise: _Alternatively, you can replace each dot in the scatterplot with the corresponding instances class (a digit from 0 to 9), or even plot scaled-down versions of the digit images themselves (if you plot all digits, the visualization will be too cluttered, so you should either draw a random sample or plot an instance only if no other instance has already been plotted at a close distance). You should get a nice visualization with well-separated clusters of digits._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e68570a",
   "metadata": {},
   "source": [
    "Let's create a `plot_digits()` function that will draw a scatterplot (similar to the above scatterplots) plus write colored digits, with a minimum distance guaranteed between these digits. If the digit images are provided, they are plotted instead. This implementation was inspired from one of Scikit-Learn's excellent examples ([plot_lle_digits](https://scikit-learn.org/stable/auto_examples/manifold/plot_lle_digits.html), based on a different digit dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923b26dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from matplotlib.offsetbox import AnnotationBbox, OffsetImage\n",
    "\n",
    "def plot_digits(X, y, min_distance=0.04, images=None, figsize=(13, 10)):\n",
    "    # Let's scale the input features so that they range from 0 to 1\n",
    "    X_normalized = MinMaxScaler().fit_transform(X)\n",
    "    # Now we create the list of coordinates of the digits plotted so far.\n",
    "    # We pretend that one is already plotted far away at the start, to\n",
    "    # avoid `if` statements in the loop below\n",
    "    neighbors = np.array([[10., 10.]])\n",
    "    # The rest should be self-explanatory\n",
    "    plt.figure(figsize=figsize)\n",
    "    cmap = plt.cm.jet\n",
    "    digits = np.unique(y)\n",
    "    for digit in digits:\n",
    "        plt.scatter(X_normalized[y == digit, 0], X_normalized[y == digit, 1],\n",
    "                    c=[cmap(float(digit) / 9)], alpha=0.5)\n",
    "    plt.axis(\"off\")\n",
    "    ax = plt.gca()  # get current axes\n",
    "    for index, image_coord in enumerate(X_normalized):\n",
    "        closest_distance = np.linalg.norm(neighbors - image_coord, axis=1).min()\n",
    "        if closest_distance > min_distance:\n",
    "            neighbors = np.r_[neighbors, [image_coord]]\n",
    "            if images is None:\n",
    "                plt.text(image_coord[0], image_coord[1], str(int(y[index])),\n",
    "                         color=cmap(float(y[index]) / 9),\n",
    "                         fontdict={\"weight\": \"bold\", \"size\": 16})\n",
    "            else:\n",
    "                image = images[index].reshape(28, 28)\n",
    "                imagebox = AnnotationBbox(OffsetImage(image, cmap=\"binary\"),\n",
    "                                          image_coord)\n",
    "                ax.add_artist(imagebox)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b80b2f",
   "metadata": {},
   "source": [
    "Let's try it! First let's show colored digits (not images), for all 5,000 images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db642d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_digits(X_reduced, y_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6a69f5",
   "metadata": {},
   "source": [
    "Well that's okay, but not that beautiful. Let's try with the digit images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e305c735",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_digits(X_reduced, y_sample, images=X_sample, figsize=(35, 25))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7acde12",
   "metadata": {},
   "source": [
    "That's nicer! Now let's focus on just the 3s and the 5s:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9c5823",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_digits(X_subset_reduced, y_subset, images=X_subset, figsize=(22, 22))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe89ddd4",
   "metadata": {},
   "source": [
    "Notice how similar-looking 4s are grouped together. For example, the 4s get more and more inclined as they approach the top of the figure. The inclined 9s are also closer to the top. Some 4s really do look like 9s, and vice versa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ea981c",
   "metadata": {},
   "source": [
    "Exercise: _Try using other dimensionality reduction algorithms such as PCA, LLE, or MDS and compare the resulting visualizations._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6c84ef",
   "metadata": {},
   "source": [
    "Let's start with PCA. We will also time how long it takes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8975dc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2, random_state=42)\n",
    "%time X_pca_reduced = pca.fit_transform(X_sample)\n",
    "plot_digits(X_pca_reduced, y_sample)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50959265",
   "metadata": {},
   "source": [
    "Wow, PCA is blazingly fast! But although we do see a few clusters, there's way too much overlap. Let's try LLE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523a729e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lle = LocallyLinearEmbedding(n_components=2, random_state=42)\n",
    "%time X_lle_reduced = lle.fit_transform(X_sample)\n",
    "plot_digits(X_lle_reduced, y_sample)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1aa238",
   "metadata": {},
   "source": [
    "That took more time, and yet the result does not look good at all. Let's see what happens if we apply PCA first, preserving 95% of the variance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ce0894",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_lle = make_pipeline(PCA(n_components=0.95),\n",
    "                        LocallyLinearEmbedding(n_components=2, random_state=42))\n",
    "\n",
    "%time X_pca_lle_reduced = pca_lle.fit_transform(X_sample)\n",
    "plot_digits(X_pca_lle_reduced, y_sample)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196219d8",
   "metadata": {},
   "source": [
    "The result is more or less as bad, but this time training was a bit faster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bec7afc",
   "metadata": {},
   "source": [
    "Let's try MDS:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a049c5",
   "metadata": {},
   "source": [
    "**Warning**, the following cell will take about 10-30 minutes to run, depending on your hardware:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7311a515",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time X_mds_reduced = MDS(n_components=2, random_state=42).fit_transform(X_sample)\n",
    "plot_digits(X_mds_reduced, y_sample)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896b630d",
   "metadata": {},
   "source": [
    "Meh. This does not look great, all clusters overlap too much. Let's try with PCA first, perhaps it will be faster?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede4a870",
   "metadata": {},
   "source": [
    "**Warning**, the following cell will take about 10-30 minutes to run, depending on your hardware:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890e5b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_mds = make_pipeline(PCA(n_components=0.95, random_state=42),\n",
    "                        MDS(n_components=2, random_state=42))\n",
    "\n",
    "%time X_pca_mds_reduced = pca_mds.fit_transform(X_sample)\n",
    "plot_digits(X_pca_mds_reduced, y_sample)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d338e3",
   "metadata": {},
   "source": [
    "Same result, and not faster: PCA did not help in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0e8c40",
   "metadata": {},
   "source": [
    "Let's try LDA now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a77a4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "lda = LinearDiscriminantAnalysis(n_components=2)\n",
    "%time X_lda_reduced = lda.fit_transform(X_sample, y_sample)\n",
    "plot_digits(X_lda_reduced, y_sample, figsize=(12, 12))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2f4d8c",
   "metadata": {},
   "source": [
    "This one is very fast, and it looks nice at first, until you realize that several clusters overlap severely."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271a09f0",
   "metadata": {},
   "source": [
    "Well, it's pretty clear that t-SNE won this little competition, wouldn't you agree?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb74884",
   "metadata": {},
   "source": [
    "And that's all for today, I hope you enjoyed this chapter!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad6c55c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2db421",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
